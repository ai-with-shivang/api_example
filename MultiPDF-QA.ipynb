{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pdfplumber langchain faiss-cpu sentence-transformers ollama langchain-text-splitters langchain-community\n",
    "\n",
    "import subprocess\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 1: Extract text from PDFs\n",
    "# -------------------------------\n",
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extracts text from a single PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 2: Build vectorstore from multiple PDFs\n",
    "# -------------------------------\n",
    "def build_vectorstore_from_pdfs(pdf_paths):\n",
    "    \"\"\"Builds a single FAISS vectorstore from multiple PDFs.\"\"\"\n",
    "    all_chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "    for path in pdf_paths:\n",
    "        text = extract_pdf_text(path)\n",
    "        chunks = splitter.split_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_texts(all_chunks, embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 3: Ask question across PDFs\n",
    "# -------------------------------\n",
    "def ask_question(vectorstore, query, k=3):\n",
    "    \"\"\"Searches across all PDFs in the vectorstore and returns only Q&A.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    prompt = f\"Answer the question based on the PDFs:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", \"llama3\"],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"replace\",\n",
    "        capture_output=True\n",
    "    )\n",
    "    print(\"\\n--- Question ---\")\n",
    "    print(query)\n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Example usage\n",
    "# -------------------------------\n",
    "pdf_paths = [\n",
    "    r\"C:\\Users\\2shiv\\Downloads\\Shivang_Soni_Enhanced_CV.pdf\",\n",
    "    r\"C:\\Users\\2shiv\\Downloads\\Research_Paper_On_Artificial_Intelligence_And_Its.pdf\"\n",
    "]\n",
    "\n",
    "vectorstore = build_vectorstore_from_pdfs(pdf_paths)\n",
    "\n",
    "# Ask a question across both PDFs\n",
    "ask_question(vectorstore, \"Summarize Shivang Soni's skills and the main AI research topics.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
